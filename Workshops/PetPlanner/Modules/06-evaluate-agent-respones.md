# ğŸ• Module 6: Evaluate Agent Responses

Evaluating responses ensures your agent meets expectations â€” helpful, playful, and reliable â€” while handling edge cases gracefully. Your goal is to Assess your Pet Plannerâ€™s performance.

## ğŸ’¬ Sample Prompt

Add evaluation to my agent using the Azure AI Evaluation SDK.

## ğŸ§© Instructions

1. TBD
1. TBD
1. TBD
1. TBD

## ğŸ” Whatâ€™s Happening

Copilot compares your agentâ€™s responses against best practices and performance criteria, surfacing improvements in tone, relevance, or correctness.

GitHub Copilot calls 2 tools:

- Evaluation Planner
- Get Evaluation Agent Runner Best Practices
- Get Evaluation Code Generation Best Practices
- Get AI Model Guidance

## âœ… Checkpoint

You now have synthetic data ready to evaluate your Pet Plannerâ€™s responses. You should also have an evaluation script that runs the recommended evaluators, and results from your latest evaluation run.

## ğŸ¾ Workshop Complete

ğŸ‰ Youâ€™ve built, connected, and optimized your Pet Planner agent â€” ready to sniff out the perfect playdate!