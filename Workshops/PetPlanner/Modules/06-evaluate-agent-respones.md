# ğŸ• Module 6: Evaluate Agent Responses

Evaluating responses ensures your agent meets expectations â€” helpful, playful, and reliable â€” while handling edge cases gracefully. Your goal is to Assess your Pet Plannerâ€™s performance.

## ğŸ§© Instructions

1. Open the GitHub Copilot chat window.
1. In the chat window, enter the **GitHub Copilot Prompt** provided below and submit.
1. Review the response from GitHub Copilot. Given the non-deterministic nature of language models, responses will vary.
1. If GitHub Copilot inquiries whether to create a dataset with queries and responses, respond: `Yes, only create 3 rows of data`.
1. After GitHub Copilot completes it's task of creating a test dataset and the evaluation file, run the evaluation file in the terminal.
1. Review the evaluation results.

## ğŸ’¬ GitHub Copilot Prompt

Add evaluation to my agent using the Azure AI Evaluation SDK.

## ğŸ” Whatâ€™s Happening

Copilot compares your agentâ€™s responses against best practices and performance criteria, surfacing improvements in tone, relevance, or correctness.

GitHub Copilot calls 2 tools:

- Evaluation Planner
- Get Evaluation Agent Runner Best Practices
- Get Evaluation Code Generation Best Practices
- Get AI Model Guidance

## âœ… Checkpoint

You now have synthetic data ready to evaluate your Pet Plannerâ€™s responses. You should also have an evaluation script that runs the recommended evaluators, and results from your latest evaluation run.

## ğŸ¾ Workshop Complete

ğŸ‰ Youâ€™ve built, connected, and optimized your Pet Planner agent â€” ready to sniff out the perfect playdate!