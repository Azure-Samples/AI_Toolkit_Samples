{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = os.getenv('GITHUB_TOKEN')\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"text-embedding-3-small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    base_url=endpoint,\n",
    "    api_key=token,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data[0]: length=1536, [-0.00721184303984046, 0.007491494063287973, ..., 0.01611734740436077, -0.004887983202934265]\n",
      "data[1]: length=1536, [-0.003025691257789731, 0.009231699630618095, ..., 0.029947662726044655, 0.020937401801347733]\n",
      "data[2]: length=1536, [-0.013795719482004642, 0.031857650727033615, ..., 0.017506178468465805, 0.0226223636418581]\n",
      "Usage(prompt_tokens=6, total_tokens=6)\n"
     ]
    }
   ],
   "source": [
    "#Input a fixed list of phrases to get their embeddings\n",
    "\n",
    "response = client.embeddings.create(\n",
    "    input=[\"first phrase\", \"second phrase\", \"third phrase\"],\n",
    "    model=model_name,\n",
    ")\n",
    "\n",
    "for item in response.data:\n",
    "    length = len(item.embedding)\n",
    "    print(\n",
    "        f\"data[{item.index}]: length={length}, \"\n",
    "        f\"[{item.embedding[0]}, {item.embedding[1]}, \"\n",
    "        f\"..., {item.embedding[length-2]}, {item.embedding[length-1]}]\"\n",
    "    )\n",
    "print(response.usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract data from PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader,PyMuPDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir=\"docs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the documents from the directory\n",
    "\n",
    "def load_docs(dir):\n",
    "    loader=DirectoryLoader(dir,loader_cls=PyMuPDFLoader,use_multithreading=True,max_concurrency=128,show_progress=True,silent_errors=True)\n",
    "    documents=loader.load()\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the documents into chunks\n",
    "\n",
    "def split_docs(documents,chunk_size=1000,chunk_overlap=100):\n",
    "    text_splitter=RecursiveCharacterTextSplitter(chunk_size=chunk_size,chunk_overlap=chunk_overlap)\n",
    "    docs=text_splitter.split_documents(documents)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 30.30it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents=load_docs(dir)\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "doc=split_docs(documents)\n",
    "print(len(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#langchain client OpenAIEmbeddings\n",
    "\n",
    "clientopen = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-large\",\n",
    "    base_url=endpoint,\n",
    "    api_key=token,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to=Chroma.from_documents(documents=doc,embedding=clientopen,persist_directory='./ai-toolkit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"What is AI Toolkit?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'author': 'Shreyan Fernandes', 'creationDate': \"D:20240708201313+05'30'\", 'creationdate': '2024-07-08T20:13:13+05:30', 'creator': 'Microsoft® Word 2019', 'file_path': 'docs\\\\AIToolkit.pdf', 'format': 'PDF 1.7', 'keywords': '', 'modDate': \"D:20240708201313+05'30'\", 'moddate': '2024-07-08T20:13:13+05:30', 'page': 0, 'producer': 'Microsoft® Word 2019', 'source': 'docs\\\\AIToolkit.pdf', 'subject': '', 'title': '', 'total_pages': 6, 'trapped': ''}, page_content='The AI Toolkit for Visual Studio Code (VS Code) was previously known as Windows AI Studio. The \\nextension has been renamed to reflect the focus on enabling AI development in VS Code across \\nplatforms. \\n \\nThe AI Toolkit for VS Code (AI Toolkit) is a VS Code extension that enables you to: \\n \\nDownload and run AI models locally. The AI Toolkit provides out-of-the-box access to highly \\noptimized models for the following platforms and hardware: \\n• \\nWindows 11 running with DirectML acceleration \\n• \\nWindows 11 running directly on the CPU \\n• \\nLinux with NVIDIA acceleration \\n• \\nLinux running directly on the CPU \\n• \\nTest models in an intuitive playground or in your application with a REST API. \\nFine-tune your AI model - locally or in the cloud (on a virtual machine) - to create new skills, improve \\nreliability of responses, set the tone and format of the response. The AI Toolkit provides a guided \\nwalkthrough to fine-tune popular small-language models (SLMs) - like Phi-3 and Mistral.'), Document(metadata={'author': 'Shreyan Fernandes', 'creationDate': \"D:20240708201313+05'30'\", 'creationdate': '2024-07-08T20:13:13+05:30', 'creator': 'Microsoft® Word 2019', 'file_path': 'docs\\\\AIToolkit.pdf', 'format': 'PDF 1.7', 'keywords': '', 'modDate': \"D:20240708201313+05'30'\", 'moddate': '2024-07-08T20:13:13+05:30', 'page': 0, 'producer': 'Microsoft® Word 2019', 'source': 'docs\\\\AIToolkit.pdf', 'subject': '', 'title': '', 'total_pages': 6, 'trapped': ''}, page_content='The AI Toolkit for Visual Studio Code (VS Code) was previously known as Windows AI Studio. The \\nextension has been renamed to reflect the focus on enabling AI development in VS Code across \\nplatforms. \\n \\nThe AI Toolkit for VS Code (AI Toolkit) is a VS Code extension that enables you to: \\n \\nDownload and run AI models locally. The AI Toolkit provides out-of-the-box access to highly \\noptimized models for the following platforms and hardware: \\n• \\nWindows 11 running with DirectML acceleration \\n• \\nWindows 11 running directly on the CPU \\n• \\nLinux with NVIDIA acceleration \\n• \\nLinux running directly on the CPU \\n• \\nTest models in an intuitive playground or in your application with a REST API. \\nFine-tune your AI model - locally or in the cloud (on a virtual machine) - to create new skills, improve \\nreliability of responses, set the tone and format of the response. The AI Toolkit provides a guided \\nwalkthrough to fine-tune popular small-language models (SLMs) - like Phi-3 and Mistral.'), Document(metadata={'author': 'Shreyan Fernandes', 'creationDate': \"D:20240708201313+05'30'\", 'creationdate': '2024-07-08T20:13:13+05:30', 'creator': 'Microsoft® Word 2019', 'file_path': 'docs\\\\AIToolkit.pdf', 'format': 'PDF 1.7', 'keywords': '', 'modDate': \"D:20240708201313+05'30'\", 'moddate': '2024-07-08T20:13:13+05:30', 'page': 0, 'producer': 'Microsoft® Word 2019', 'source': 'docs\\\\AIToolkit.pdf', 'subject': '', 'title': '', 'total_pages': 6, 'trapped': ''}, page_content='The AI Toolkit for Visual Studio Code (VS Code) was previously known as Windows AI Studio. The \\nextension has been renamed to reflect the focus on enabling AI development in VS Code across \\nplatforms. \\n \\nThe AI Toolkit for VS Code (AI Toolkit) is a VS Code extension that enables you to: \\n \\nDownload and run AI models locally. The AI Toolkit provides out-of-the-box access to highly \\noptimized models for the following platforms and hardware: \\n• \\nWindows 11 running with DirectML acceleration \\n• \\nWindows 11 running directly on the CPU \\n• \\nLinux with NVIDIA acceleration \\n• \\nLinux running directly on the CPU \\n• \\nTest models in an intuitive playground or in your application with a REST API. \\nFine-tune your AI model - locally or in the cloud (on a virtual machine) - to create new skills, improve \\nreliability of responses, set the tone and format of the response. The AI Toolkit provides a guided \\nwalkthrough to fine-tune popular small-language models (SLMs) - like Phi-3 and Mistral.'), Document(metadata={'author': 'Shreyan Fernandes', 'creationDate': \"D:20240708201313+05'30'\", 'creationdate': '2024-07-08T20:13:13+05:30', 'creator': 'Microsoft® Word 2019', 'file_path': 'docs\\\\AIToolkit.pdf', 'format': 'PDF 1.7', 'keywords': '', 'modDate': \"D:20240708201313+05'30'\", 'moddate': '2024-07-08T20:13:13+05:30', 'page': 0, 'producer': 'Microsoft® Word 2019', 'source': 'docs\\\\AIToolkit.pdf', 'subject': '', 'title': '', 'total_pages': 6, 'trapped': ''}, page_content=\"walkthrough to fine-tune popular small-language models (SLMs) - like Phi-3 and Mistral. \\nDeploy your AI feature either to the cloud or with an application that runs on a device. \\n \\nThe AI Toolkit for VS Code (AI Toolkit) is a VS Code extension that enables you to download, test, fine-\\ntune, and deploy AI models with your apps or the cloud. For more information, see the AI Toolkit \\noverview. \\n \\nIn this article, you'll learn how to: \\n \\n• \\nInstall the AI Toolkit for VS Code \\n• \\nDownload a model from the catalog \\n• \\nRun the model locally using the playground \\n• \\nIntegrate an AI model into your application using REST or the ONNX Runtime \\nPrerequisites \\nVS Code must be installed. For more information, see Download VS Code and Getting started with VS \\nCode. \\nInstall \\nThe AI Toolkit is available in the Visual Studio Marketplace and can be installed like any other VS \\nCode extension. If you're unfamiliar with installing VS Code extensions, follow these steps: \\n \\n•\")]\n",
      "The AI Toolkit for Visual Studio Code (VS Code) was previously known as Windows AI Studio. The \n",
      "extension has been renamed to reflect the focus on enabling AI development in VS Code across \n",
      "platforms. \n",
      " \n",
      "The AI Toolkit for VS Code (AI Toolkit) is a VS Code extension that enables you to: \n",
      " \n",
      "Download and run AI models locally. The AI Toolkit provides out-of-the-box access to highly \n",
      "optimized models for the following platforms and hardware: \n",
      "• \n",
      "Windows 11 running with DirectML acceleration \n",
      "• \n",
      "Windows 11 running directly on the CPU \n",
      "• \n",
      "Linux with NVIDIA acceleration \n",
      "• \n",
      "Linux running directly on the CPU \n",
      "• \n",
      "Test models in an intuitive playground or in your application with a REST API. \n",
      "Fine-tune your AI model - locally or in the cloud (on a virtual machine) - to create new skills, improve \n",
      "reliability of responses, set the tone and format of the response. The AI Toolkit provides a guided \n",
      "walkthrough to fine-tune popular small-language models (SLMs) - like Phi-3 and Mistral.\n"
     ]
    }
   ],
   "source": [
    "db1=Chroma(persist_directory='./ai-toolkit',embedding_function=clientopen)\n",
    "results=db1.similarity_search(query)\n",
    "print(results)\n",
    "print(results[0].page_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
